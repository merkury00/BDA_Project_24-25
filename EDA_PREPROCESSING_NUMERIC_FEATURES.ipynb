{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c168984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    when, countDistinct, count, avg,\n",
    "    split, explode, trim, lit, length,\n",
    "    current_date, row_number, regexp_replace,\n",
    "    to_date, coalesce, col,\n",
    "    year, month, dayofmonth, array_contains, min, max, year\n",
    ")\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4758a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/FileStore/tables/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce17346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"/FileStore/tables/Imdb_Movie_Dataset-4.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "# quote <-- Important to handle commas correctly\n",
    "df = (\n",
    "    spark.read.format(file_type)\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", first_row_is_header)\n",
    "    .option(\"sep\", delimiter)\n",
    "    .option(\"quote\", '\"')\n",
    "    .option(\"escape\", '\"')\n",
    "    .load(file_location)\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3025332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the total number of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5cfe58",
   "metadata": {},
   "source": [
    "%md\n",
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f8747b",
   "metadata": {},
   "source": [
    "%md\n",
    "**Can we predict the box office revenue of a movie before it is released?**\n",
    "\n",
    "This project explores that question by building a machine learning regression model using Apache Spark MLlib to predict the revenue of a movie based on features such as budget, genre, popularity, vote average, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bda34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the type of each feature\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a316dc5b",
   "metadata": {},
   "source": [
    "%md\n",
    "Here we can observe that some features have entries with an incorrect or inappropriate data type for what they represent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7604052f",
   "metadata": {},
   "source": [
    "%md\n",
    "## Dataset Schema with Correct Types and Columns Description\n",
    "\n",
    "| Column                  | Current Type | Correct Type      | Column Description                                                                          |\n",
    "|-------------------------|--------------|-------------------|---------------------------------------------------------------------------------------------|\n",
    "| id                      | integer      | integer           |A unique identifier for each movie                                                           |\n",
    "| title                   | string       | string            |The name of the movie                                                                        |\n",
    "| vote_average            | string       | **double** (float)|The average rating the movie has received from users (on a scale, typically from 0 to 10)    |\n",
    "| vote_count              | string       | **integer**       |The total number of votes or ratings submitted for the movie                                 |\n",
    "| status                  | string       | string            |The current state of the movie (e.g., \"Released,\" \"Post-Production\")                         |\n",
    "| release_date            | string       | **date**          |The date when the movie was officially released                                              |\n",
    "| revenue                 | string       | **long** (integer)|The total earnings the movie made (usually in USD)                                           |\n",
    "| runtime                 | string       | **integer**       |The duration of the movie in minutes                                                         |\n",
    "| adult                   | string       | **boolean**       |Indicates whether the movie is classified as adult content (e.g., \"True\" or \"False\")          |\n",
    "| budget                  | string       | **long** (integer)|The total cost of producing the movie (usually in USD)                                       |\n",
    "| imdb_id                 | string       | string            |The unique identifier for the movie on IMDb (Internet Movie Database)                        |\n",
    "| original_language       | string       | string            |The language in which the movie was originally produced (e.g., \"en\" for English)             |\n",
    "| original_title          | string       | string            |The original title of the movie in its native language                                       |\n",
    "| overview                | string       | string            |A brief summary or description of the movie's plot                                           |\n",
    "| popularity              | string       | **double** (float)|A metric indicating how popular the movie is (typically based on views, searches, or ratings)|\n",
    "| tagline                 | string       | string            |A short phrase or slogan associated with the movie                                           |\n",
    "| genres                  | string       | string   |The categories or genres the movie belongs to (e.g., Action, Comedy, Drama)                  |\n",
    "| production_companies    | string       | string   |production_companies: The names of the companies involved in producing the movie             |\n",
    "| production_countries    | string       | string   |The countries where the movie was produced                                                   |\n",
    "| spoken_languages        | string       | string   |The languages spoken in the movie                                                            |\n",
    "| keywords                | string       | string   |Important terms or phrases associated with the movie, often used for categorization or search|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081ae663",
   "metadata": {},
   "source": [
    "%md\n",
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column\n",
    "def missing_values_table_spark(df):\n",
    "    \"\"\"\n",
    "    Calculates the total number and percentage of missing (null) values \n",
    "    for each column in a PySpark DataFrame.\n",
    "\n",
    "    Returns a Pandas DataFrame with columns:\n",
    "    - 'Column': column name\n",
    "    - 'Missing Values': count of missing values\n",
    "    - '% of Total Values': percentage of missing values\n",
    "    \n",
    "    Only columns with missing values are included, sorted in descending order.\n",
    "    \"\"\"\n",
    "    # Calculate the total missing values for each column\n",
    "    mis_val = df.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "\n",
    "    # Convert to Pandas for easier handling\n",
    "    mis_val_pd = mis_val.toPandas().transpose()\n",
    "\n",
    "    # Calculate the percentage of missing values for each column\n",
    "    mis_val_percent = (mis_val_pd[0] / df.count()) * 100\n",
    "\n",
    "    # Create a new table combining count and percentage\n",
    "    mis_val_table = pd.concat([mis_val_pd, mis_val_percent], axis=1)\n",
    "    mis_val_table.columns = ['Missing Values', '% of Total Values']\n",
    "\n",
    "    # Keep only columns with >0% missing, sort descending, round\n",
    "    mis_val_table = (\n",
    "        mis_val_table[mis_val_table['% of Total Values'] > 0]\n",
    "        .sort_values('% of Total Values', ascending=False)\n",
    "        .round(1)\n",
    "    )\n",
    "\n",
    "    # Reset index so that original column names become a column\n",
    "    mis_val_table = mis_val_table.reset_index().rename(columns={'index': 'Column'})\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"Your selected dataframe has {len(df.columns)} columns.\\n\"\n",
    "          f\"There are {mis_val_table.shape[0]} columns that have missing values.\")\n",
    "\n",
    "    return mis_val_table\n",
    "\n",
    "# Usage\n",
    "missing_values = missing_values_table_spark(df)\n",
    "display(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a8a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_missing_values(mis_val_table, title):\n",
    "    \"\"\"\n",
    "    Plots the percentage of missing values per feature, showing feature names on the x-axis.\n",
    "    Assumes `mis_val_table` has a column 'Column' with the feature names and\n",
    "    '% of Total Values' with the missing-value percentages.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Use the 'Column' column for x-labels\n",
    "    sns.barplot(\n",
    "        data=mis_val_table,\n",
    "        x='Column',\n",
    "        y='% of Total Values'\n",
    "    )\n",
    "\n",
    "    # Rotate the x labels for better readability\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Set plot labels and title\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('% of Total Values')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot the missing values with feature names shown\n",
    "plot_missing_values(missing_values, title='Percentage of Missing Values by Feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448288ca",
   "metadata": {},
   "source": [
    "%md\n",
    "While exploring the dataset, we quickly noticed that some columns have a large amount of missing data. For example, about 85% of the values in the `tagline` column and around 72% in `keywords` are missing. These high percentages suggest that these fields may not be very useful for our analysis. Considering what each variable represents and how relevant it is to our goals, we can start identifying which features are likely to bring little value and could be safely removed. This helps us simplify the dataset and minimize noise, making the overall analysis more efficient and focused."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf00868",
   "metadata": {},
   "source": [
    "%md\n",
    "## Zero Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ec85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated function to include a 'Column' column instead of using the index\n",
    "def zero_values_table_spark(df):\n",
    "    \"\"\"\n",
    "    Calculates the total number and percentage of zero values \n",
    "    for each column in a PySpark DataFrame.\n",
    "\n",
    "    Returns a Pandas DataFrame with columns:\n",
    "    - 'Column': column name\n",
    "    - 'Zero Values': count of zero values\n",
    "    - '% of Total Values': percentage of zero values\n",
    "\n",
    "    Only columns with zero values are included, sorted in descending order.\n",
    "    \"\"\"\n",
    "    # For each column, count how many values equal zero\n",
    "    zero_counts = df.select([F.sum(F.when(F.col(c) == 0, 1).otherwise(0)).alias(c) for c in df.columns])\n",
    "\n",
    "    # Convert result to Pandas for easier manipulation\n",
    "    zero_counts_pd = zero_counts.toPandas().transpose()\n",
    "\n",
    "    # Compute percentage\n",
    "    row_count = df.count()\n",
    "    zero_percent = (zero_counts_pd[0] / row_count) * 100\n",
    "\n",
    "    # Create summary table\n",
    "    zero_table = pd.concat([zero_counts_pd, zero_percent], axis=1)\n",
    "    zero_table.columns = ['Zero Values', '% of Total Values']\n",
    "\n",
    "    # Keep only columns with zeros, sort and round\n",
    "    zero_table = (\n",
    "        zero_table[zero_table['Zero Values'] > 0]\n",
    "        .sort_values('% of Total Values', ascending=False)\n",
    "        .round(1)\n",
    "    )\n",
    "\n",
    "    # Reset index so column names appear as a column\n",
    "    zero_table = zero_table.reset_index().rename(columns={'index': 'Column'})\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"Your selected dataframe has {len(df.columns)} columns.\\n\"\n",
    "          f\"There are {zero_table.shape[0]} columns that have zero values.\")\n",
    "\n",
    "    return zero_table\n",
    "\n",
    "# Use the function\n",
    "zero_values = zero_values_table_spark(df)\n",
    "display(zero_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a8ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_zero_values(zero_table, title):\n",
    "    \"\"\"\n",
    "    Plots the percentage of zero values per feature, using the 'Column' column\n",
    "    for the x-axis labels.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    sns.barplot(\n",
    "        data=zero_table,\n",
    "        x='Column',\n",
    "        y='% of Total Values'\n",
    "    )\n",
    "\n",
    "    # Rotate x-labels for readability\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('% of Total Values')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# print\n",
    "plot_zero_values(zero_values, title='Percentage of Zero Values by Feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b6c384",
   "metadata": {},
   "source": [
    "%md\n",
    "From this analysis of zero values, we can clearly see that two of the most critical columns for our originally defined problem, `revenue` and `budget`, contain an overwhelming proportion of zero entries. In fact, `revenue`, which we had considered as our potential target variable, has 98% zero values, while `budget` has nearly 95%. These figures are highly problematic, as they can significantly compromise the reliability of any predictive modeling. Therefore, one of our key next steps will be to carefully handle these zero values, either by imputing our kick these obsrvations of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48cbde",
   "metadata": {},
   "source": [
    "%md\n",
    "## Duplicated Records (ID and IMBD_ID features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3619821f",
   "metadata": {},
   "source": [
    "%md\n",
    "Our dataset contains two main identifier columns: `id` and `imdb_id`. Because of this, it's possible that duplicate values exist in either of them. In the following steps, we perform a detailed analysis to check for duplicate observations in the dataset, and, if any are found, we inspect their nature by looking at specific examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d9d839",
   "metadata": {},
   "source": [
    "%md\n",
    "for `id` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9eb67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total and distinct counts\n",
    "total_rows = df.count()\n",
    "unique_id = df.select(countDistinct(\"id\")).collect()[0][0]\n",
    "\n",
    "# Calculate duplicated counts\n",
    "duplicated_id = total_rows - unique_id\n",
    "\n",
    "# Print results\n",
    "print(f\"Total duplicated IDs: {duplicated_id}\")\n",
    "\n",
    "# Display examples of duplicated 'id' rows\n",
    "dup_id_vals = (\n",
    "    df\n",
    "    .groupBy(\"id\")\n",
    "    .count()\n",
    "    .filter(col(\"count\") > 1)\n",
    "    .select(\"id\")\n",
    ")\n",
    "dup_id_rows = (\n",
    "    df\n",
    "    .join(dup_id_vals, on=\"id\", how=\"inner\")\n",
    "    .orderBy(\"id\")\n",
    ")\n",
    "print(\"Examples of duplicated 'id' rows:\")\n",
    "display(dup_id_rows.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1707bd76",
   "metadata": {},
   "source": [
    "%md\n",
    "In this part of the analysis, we found 794 records with duplicated values in the `id` column. By inspecting these duplicates alongside their corresponding columns, we noticed that many of the repeated entries contain a large number of null values in the remaining fields. This suggests that these duplicates are most likely unnecessary entries. possibly introduced by mistake during data collection or integration. Given the high number of missing values in these cases, a sensible next step will be to remove them, keeping only the most complete version of each duplicated record to ensure data quality and consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd41896",
   "metadata": {},
   "source": [
    "%md\n",
    "for `imdb_id` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate counts for non-null imdb_id values\n",
    "non_null_rows = df.filter(col(\"imdb_id\").isNotNull()).count()\n",
    "unique_non_null = (\n",
    "    df\n",
    "    .filter(col(\"imdb_id\").isNotNull())\n",
    "    .select(countDistinct(\"imdb_id\"))\n",
    "    .collect()[0][0]\n",
    ")\n",
    "duplicated_imdb_id = non_null_rows - unique_non_null\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total non-null imdb_id rows: {non_null_rows}\")\n",
    "print(f\"Total duplicated IMDB IDs (excluding nulls): {duplicated_imdb_id}\")\n",
    "\n",
    "# Display examples of duplicated 'imdb_id' rows\n",
    "dup_imdb_vals = (\n",
    "    df\n",
    "    .filter(col(\"imdb_id\").isNotNull())\n",
    "    .groupBy(\"imdb_id\")\n",
    "    .count()\n",
    "    .filter(col(\"count\") > 1)\n",
    "    .select(\"imdb_id\")\n",
    ")\n",
    "\n",
    "dup_imdb_rows = (\n",
    "    df\n",
    "    .join(dup_imdb_vals, on=\"imdb_id\", how=\"inner\")\n",
    "    .orderBy(\"imdb_id\")\n",
    ")\n",
    "\n",
    "print(\"Examples of duplicated 'imdb_id' rows:\")\n",
    "display(dup_imdb_rows.limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522d1aed",
   "metadata": {},
   "source": [
    "%md\n",
    "As previously noted, the `imdb_id` column contains a high percentage of missing values. To accurately check for duplicates in this feature, we focused only on the non-null entries. Within this subset, we identified 1,160 duplicate records. Just like with the duplicated `id` values, our next step will be to remove these redundant entries by comparing the amount of missing data across the duplicated rows, keeping the most complete version in each case. This will help us clean the dataset and reduce unnecessary noise caused by incomplete or inconsistent records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df9b37",
   "metadata": {},
   "source": [
    "%md\n",
    "## Summary\n",
    "\n",
    "### Missing Values\n",
    "These columns have a significant number of null entries:\n",
    "- `tagline` (85.4% missing)  \n",
    "- `keywords` (72.1% missing)  \n",
    "- `production_companies` (54.2% missing)  \n",
    "- `imdb_id` (46.5% missing)  \n",
    "- `production_countries` (44.2% missing)  \n",
    "- `spoken_languages` (~42% missing)  \n",
    "- `genres` (39.6% missing)  \n",
    "- `overview` (20.6% missing)  \n",
    "- `release_date` (17.3% missing)\n",
    "\n",
    "### Zero Values\n",
    "The following columns contain a substantial fraction of zeros:\n",
    "- `revenue` (98% zeros)  \n",
    "- `budget` (94.6% zeros)  \n",
    "- `adult` (90.7% zeros)  \n",
    "- `vote_average` (66.6% zeros)  \n",
    "- `vote_count` (66.5% zeros)  \n",
    "- `runtime` (28% zeros)  \n",
    "- `popularity` (13.2% zeros)\n",
    "\n",
    "### Duplicated Records\n",
    "The dataset contains 794 duplicate entries with the same `id`, and also several duplicates based on `imdb_id`, many of which have a high number of null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a06b5c1",
   "metadata": {},
   "source": [
    "%md\n",
    "We will work with copies of our DataFrame up to each checkpoint to ensure that no unintended changes are made to the original dataset. This also helps avoid rerunning the entire pipeline multiple times whenever we want to test the behavior of a specific code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c577e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe copy of the original DataFrame\n",
    "df_copy1 = df.select(\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064ad6d9",
   "metadata": {},
   "source": [
    "%md\n",
    "First of all, we need to fix the data types of some features, otherwise they won't be useful for our project modeling.\n",
    "Some features needs an additional analysis, we will correct their data types after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a85a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy1 = df_copy1.withColumn(\n",
    "        \"vote_average\", col(\"vote_average\").cast(\"double\")\n",
    "    ).withColumn(\n",
    "        \"vote_count\", col(\"vote_count\").cast(\"integer\")\n",
    "    ).withColumn(\n",
    "        \"revenue\", col(\"revenue\").cast(\"long\")\n",
    "    ).withColumn(\n",
    "        \"runtime\", col(\"runtime\").cast(\"integer\")\n",
    "    ).withColumn(\n",
    "        \"adult\", col(\"adult\").cast(\"boolean\")\n",
    "    ).withColumn(\n",
    "        \"budget\", col(\"budget\").cast(\"long\")\n",
    "    ).withColumn(\n",
    "        \"popularity\", col(\"popularity\").cast(\"double\")\n",
    "    )\n",
    "\n",
    "\n",
    "# Verify that the types have been updated\n",
    "df_copy1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ceed2e",
   "metadata": {},
   "source": [
    "%md\n",
    "## Fixing Duplicate Records (ID and IMBD_ID features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553c8666",
   "metadata": {},
   "source": [
    "%md\n",
    "Here, as previously mentioned in the section where we identified the presence of outliers, we will now deal with them. In order to properly handle the duplicates found in our two identifier columns, we first need to ensure that missing values are correctly represented. Specifically, we will count and replace, if present, the string *'None'* with the actual *None* (*null*) value in the `imdb_id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b54556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many \"None\" strings are in imdb_id\n",
    "none_count = df_copy1.filter(col(\"imdb_id\") == \"None\").count()\n",
    "\n",
    "# Replace \"None\" strings with actual nulls\n",
    "df_copy1 = df_copy1.withColumn(\n",
    "    \"imdb_id\",\n",
    "    when(col(\"imdb_id\") == \"None\", None)\n",
    "      .otherwise(col(\"imdb_id\"))\n",
    ")\n",
    "\n",
    "# Print how many values were set to null\n",
    "print(f\"Number of 'None' values replaced with null in 'imdb_id': {none_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f7e77",
   "metadata": {},
   "source": [
    "%md\n",
    "This section is where we identified the presence of duplicates, we will now deal with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca82893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates_keep_least_null(df, id_col=\"id\"):\n",
    "    \"\"\"\n",
    "    Remove duplicate rows among those with non-null `id_col`, keeping the row with the fewest null values.\n",
    "    Rows where `id_col` is null are left untouched.\n",
    "    Prints how many rows were dropped among non-null IDs.\n",
    "    \"\"\"\n",
    "    # Split into rows with and without an ID\n",
    "    non_null_df = df.filter(col(id_col).isNotNull())\n",
    "    null_df     = df.filter(col(id_col).isNull())\n",
    "    \n",
    "    # Count non-null rows before deduplication\n",
    "    orig_count = non_null_df.count()\n",
    "    \n",
    "    # For each row, count how many columns are null\n",
    "    null_count_expr = sum(\n",
    "        when(col(c).isNull(), 1).otherwise(0) for c in non_null_df.columns\n",
    "    ).alias(\"_null_count\")\n",
    "    nn_with_nulls = non_null_df.withColumn(\"_null_count\", null_count_expr)\n",
    "    \n",
    "    # Within each id group, rank rows by null_count ascending\n",
    "    window = Window.partitionBy(id_col).orderBy(col(\"_null_count\").asc())\n",
    "    ranked = nn_with_nulls.withColumn(\"_rn\", row_number().over(window))\n",
    "    \n",
    "    # Keep only the first row for each id (fewest nulls), drop helper cols\n",
    "    deduped_non_null = (\n",
    "        ranked\n",
    "        .filter(col(\"_rn\") == 1)\n",
    "        .drop(\"_null_count\", \"_rn\")\n",
    "    )\n",
    "    \n",
    "    # Count how many non-null rows were dropped\n",
    "    new_count = deduped_non_null.count()\n",
    "    dropped = orig_count - new_count\n",
    "    print(f\"Dropped {dropped} duplicate rows among non-null '{id_col}'.\")\n",
    "    \n",
    "    # Combine back with the untouched null-ID rows\n",
    "    result_df = deduped_non_null.unionByName(null_df)\n",
    "    return result_df\n",
    "\n",
    "# Use the function to deduplicate by \"id\", preserving rows with null id\n",
    "df_copy1 = drop_duplicates_keep_least_null(df_copy1, id_col=\"id\")\n",
    "\n",
    "display(df_copy1.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9157ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now drop duplicates by imdb_id\n",
    "df_copy1 = drop_duplicates_keep_least_null(df_copy1, id_col=\"imdb_id\")\n",
    "\n",
    "# Inspect a few rows to confirm\n",
    "display(df_copy1.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a451e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check - recalculate duplicates\n",
    "# Total rows\n",
    "total_rows = df_copy1.count()\n",
    "\n",
    "# id duplicates (id never null)\n",
    "unique_id = df_copy1.select(countDistinct(\"id\")).collect()[0][0]\n",
    "duplicated_id = total_rows - unique_id\n",
    "\n",
    "# imdb_id duplicates (exclude null/\"None\")\n",
    "valid_imdb = df_copy1.filter(col(\"imdb_id\").isNotNull() & (col(\"imdb_id\") != \"None\"))\n",
    "total_valid = valid_imdb.count()\n",
    "unique_imdb = valid_imdb.select(countDistinct(\"imdb_id\")).collect()[0][0]\n",
    "duplicated_imdb_id = total_valid - unique_imdb\n",
    "\n",
    "print(f\"Total duplicated IDs: {duplicated_id}\")\n",
    "print(f\"Total duplicated IMDB IDs (excluding nulls): {duplicated_imdb_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1461aafd",
   "metadata": {},
   "source": [
    "%md\n",
    "Now, `id` and `imdb_id` columns are deduplicated. Their datatype is correct, so no need for further changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0c24e9",
   "metadata": {},
   "source": [
    "%md\n",
    "## Numerical Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bddb08",
   "metadata": {},
   "source": [
    "%md\n",
    "### First cleaning (REVENUE and BUDGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cd8b07",
   "metadata": {},
   "source": [
    "%md\n",
    "As we mentioned earlier, one of the main issues with our dataset is that the `revenue` and `budget` features contain a very high percentage of zero values. Because of this, we need to address the problem, otherwise, we would be working with a massive dataset that is hard to run efficiently in Databricks and may lead to inconclusive results.\n",
    "\n",
    "We need to filter or reduce the dataset. While this is technically a big data problem (we have many entries), in practice, we'll only work with a meaningful subset. This is not only for efficiency and performance reasons but also because we want to use `revenue` as our target variable and if 98% of our dataset has zero revenue, training a model on all of it would introduce a strong bias and hurt the model's ability to learn meaningful patterns. We would essentially be training on noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277efd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of null, empty-string, and \"None\"/\"none\" counts for revenue and budget\n",
    "total = df_copy1.count()\n",
    "\n",
    "for cname in [\"revenue\", \"budget\"]:\n",
    "    nulls   = df_copy1.filter(col(cname).isNull()).count()\n",
    "    empties = df_copy1.filter((col(cname) == \"\") |(col(cname) == \" \")).count()\n",
    "    nones   = df_copy1.filter(col(cname).isin(\"None\", \"none\")).count()\n",
    "    zeros   = df_copy1.filter(col(cname) == 0).count()\n",
    "    \n",
    "    print(\n",
    "        f\"{cname}: total={total}, \"\n",
    "        f\"nulls={nulls}, \"\n",
    "        f\"empty strings={empties}, \"\n",
    "        f\"'None'/'none' strings={nones}, \"\n",
    "        f\"zeros={zeros}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a3f4e",
   "metadata": {},
   "source": [
    "%md\n",
    "Now, we will drop the rows where `revenue` == 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a928c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total rows before filtering\n",
    "initial_count = df_copy1.count()\n",
    "\n",
    "# Filter to only movies with revenue > 0\n",
    "df_copy1 = df_copy1.filter(col(\"revenue\").isNotNull() & (col(\"revenue\") > 0))\n",
    "\n",
    "# Count rows after filtering\n",
    "final_count = df_copy1.count()\n",
    "\n",
    "# Compute how many were dropped\n",
    "dropped_count = initial_count - final_count\n",
    "\n",
    "# Report\n",
    "print(f\"Dropped {dropped_count} movies with zero or null revenue.\")\n",
    "print(f\"Remaining movies for training: {final_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16242d67",
   "metadata": {},
   "source": [
    "%md\n",
    "We drop all movies with `revenue` == 0 because in our dataset those zeroes overwhelmingly represent missing or unreported box-office figures, not true earnings, and including them would force the model to learn spurious patterns tied to data availability rather than actual revenue drivers. By training only on films with known, positive revenue, we ensure the regression learns genuine relationships (e.g. between budget, popularity, genre, etc.) and avoids biasing predictions toward “zero” whenever it encounters features associated with unreleased or untracked titles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3d331",
   "metadata": {},
   "source": [
    "%md\n",
    "ploting the origianl distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b6b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter \n",
    "df_money = df_copy1.filter((col(\"budget\") > 0) & (col(\"revenue\") > 0))\n",
    "\n",
    "# Select and convert necessary columns to Pandas\n",
    "df_money_pd = df_money.select(\"budget\", \"revenue\").toPandas()\n",
    "\n",
    "# Plot original distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Budget distribution\n",
    "sns.histplot(df_money_pd['budget'], bins=100, ax=axes[0], color='orange')\n",
    "axes[0].set_title('Budget Distribution')\n",
    "axes[0].set_xlabel('Budget ($)')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Revenue distribution\n",
    "sns.histplot(df_money_pd['revenue'], bins=100, ax=axes[1], color='blue')\n",
    "axes[1].set_title('Revenue Distribution')\n",
    "axes[1].set_xlabel('Revenue ($)')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c4639",
   "metadata": {},
   "source": [
    "%md\n",
    "ploting with the log10 transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b3f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Spark DataFrame for positive budget and revenue\n",
    "df_money = df_copy1.filter((col(\"budget\") > 0) & (col(\"revenue\") > 0))\n",
    "\n",
    "# Convert to Pandas\n",
    "df_money_pd = df_money.select(\"budget\", \"revenue\").toPandas()\n",
    "\n",
    "# Apply log10 transformation\n",
    "df_money_pd['log_budget'] = np.log10(df_money_pd['budget'])\n",
    "df_money_pd['log_revenue'] = np.log10(df_money_pd['revenue'])\n",
    "\n",
    "# Plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.histplot(df_money_pd['log_budget'], bins=50, ax=axes[0], color='orange')\n",
    "axes[0].set_title('Log10 Budget Distribution')\n",
    "axes[0].set_xlabel('Log10(Budget)')\n",
    "\n",
    "sns.histplot(df_money_pd['log_revenue'], bins=50, ax=axes[1], color='blue')\n",
    "axes[1].set_title('Log10 Revenue Distribution')\n",
    "axes[1].set_xlabel('Log10(Revenue)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf70497b",
   "metadata": {},
   "source": [
    "%md\n",
    "Budget and Revenue (log₁₀) Distributions\n",
    "\n",
    "Log₁₀(Budget)\n",
    "The bulk of film budgets cluster around log₁₀(budget) ≈ 6.5–7.5, i.e. budgets of roughly \\$3 million to \\$30 million. Most budgets fall between log₁₀ ≈ 5 (≈ \\$100 k) and log₁₀ ≈ 8 (≈ \\$100 million). There is a long left‐hand tail extending down toward zero (very low‐budget or no‐budget films), and a shorter right tail tapering off beyond log₁₀ ≈ 8. Small peaks around log₁₀ ≈ 2–3 (\\$100–\\$1 000 budgets) and log₁₀ ≈ 4–5 (\\$10 000–\\$100 000) likely correspond to micro‐budget productions.\n",
    "\n",
    "Log₁₀(Revenue)\n",
    "Revenues tend to concentrate around log₁₀(revenue) ≈ 7–8, i.e. \\$10 million to \\$100 million. Revenues span from very low figures (log₁₀ ≈ 0–2, under \\$100–\\$100) up to blockbuster grosses (log₁₀ ≈ 9+, over \\$1 billion), though the extreme high end thins out sharply. The distribution is right‐skewed, with a heavy left tail of low‐earning films and a tapering right tail of big‐blockbusters. A visible bump near log₁₀ ≈ 4–5 (\\$10 000–\\$100 000) suggests a cluster of modest indie releases, and occasional spikes around log₁₀ ≈ 2 (\\$100–\\$1 000) reflect very small‐scale runs or data artifacts.\n",
    "\n",
    "Overall, both distributions are left-skewed on the log scale, with most films clustering in the mid‐range budgets and revenues, but with long tails toward both the ultra‐low and the ultra‐high ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67648c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movies with budget == 0 but revenue != 0\n",
    "zero_budget_nonzero_rev = df_copy1.filter((col(\"budget\") == 0) & (col(\"revenue\") != 0))\n",
    "count_zero_budget_nonzero_rev = zero_budget_nonzero_rev.count()\n",
    "print(f\"Movies with budget = 0 and revenue != 0: {count_zero_budget_nonzero_rev}\")\n",
    "display(zero_budget_nonzero_rev.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a7e76",
   "metadata": {},
   "source": [
    "%md\n",
    "Here we can observe that some movies had zero budget but still generated revenue. At first, this may seem strange, but it makes sense when we consider, for example, that many animated films or productions have very low or zero production costs and can still achieve substantial profits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6df55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movies with negative budget\n",
    "neg_budget_count = df_copy1.filter(col(\"budget\") < 0).count()\n",
    "print(f\"Number of movies with negative budget: {neg_budget_count}\")\n",
    "\n",
    "#  Movies with negative revenue\n",
    "neg_revenue_count = df_copy1.filter(col(\"revenue\") < 0).count()\n",
    "print(f\"Number of movies with negative revenue: {neg_revenue_count}\")\n",
    "display(df_copy1.filter(col(\"revenue\") < 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be4b7be",
   "metadata": {},
   "source": [
    "%md\n",
    "### VOTE_AVERAGE and VOTE_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of null, empty-string, and \"None\"/\"none\" counts for vote_average and vote_count\n",
    "total = df_copy1.count()\n",
    "\n",
    "for cname in [\"vote_average\", \"vote_count\"]:\n",
    "    nulls   = df_copy1.filter(col(cname).isNull()).count()\n",
    "    empties = df_copy1.filter((col(cname) == \"\") | (col(cname) == \" \")).count()\n",
    "    nones   = df_copy1.filter(col(cname).isin(\"None\", \"none\")).count()\n",
    "    \n",
    "    print(\n",
    "        f\"{cname}: total={total}, \"\n",
    "        f\"nulls={nulls}, \"\n",
    "        f\"empty strings={empties}, \"\n",
    "        f\"'None'/'none' strings={nones}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfff48c",
   "metadata": {},
   "source": [
    "%md\n",
    "IMDb’s user rating system allows you to assign any whole-number score from 1 (the lowest) up to 10 (the highest) for any movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef054700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movies with vote_average below the allowed minimum (1)\n",
    "below_allowed = df_copy1.filter(col(\"vote_average\") < 1)\n",
    "count_below = below_allowed.count()\n",
    "print(f\"Number of movies with vote_average below 1: {count_below}\")\n",
    "\n",
    "# Movies with vote_average above the allowed maximum (10)\n",
    "above_allowed = df_copy1.filter(col(\"vote_average\") > 10)\n",
    "count_above = above_allowed.count()\n",
    "print(f\"Number of movies with vote_average above 10: {count_above}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf9e2f",
   "metadata": {},
   "source": [
    "%md\n",
    "we identified nearly 4,000 records where the `vote_average` was below 1, even though the `vote_count` was not null. Since IMDb does not allow movie ratings lower than 1, we considered these values invalid. To handle this issue, we will first replace every instance, where `vote_average` == 0 with null. Later, after data split we will decide how to handle this issue further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e71127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count instances where vote_count == 0\n",
    "count_zero = df_copy1.filter(col(\"vote_count\") == 0).count()\n",
    "print(f\"Number of rows with vote_count == 0: {count_zero}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movies where vote_count == 0 but vote_average != 0\n",
    "cond = df_copy1.filter((col(\"vote_count\") == 0) & (col(\"vote_average\") != 0))\n",
    "count = cond.count()\n",
    "print(f\"Number of movies with vote_count == 0 and vote_average != 0: {count}\")\n",
    "# Display all columns for a few examples\n",
    "display(cond.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc3e616",
   "metadata": {},
   "source": [
    "%md\n",
    "Here we can see that there are 4 movies where the `vote_count` is zero, yet the `vote_average` is different from zero. This situation is clearly not possible, since the average should depend on the number of votes. Therefore, we will cast those `vote_average` to null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. count how many rows either have `vote_count`==0 or `vote_average` < 1\n",
    "flagged_count = df_copy1.filter(\n",
    "    (F.col(\"vote_count\") == 0) & \n",
    "    (F.col(\"vote_average\") < 1)\n",
    ").count()\n",
    "\n",
    "# 2. apply the transformation: set vote_average to null for those rows\n",
    "df_copy1 = df_copy1.withColumn(\n",
    "    \"vote_average\",\n",
    "    when(\n",
    "        (col(\"vote_average\") < 1) & (col(\"vote_count\") == 0),\n",
    "        lit(None).cast(\"double\")     # cast via type name\n",
    "    ).otherwise(col(\"vote_average\"))\n",
    ")\n",
    "\n",
    "# 3. count how many rows are now null\n",
    "null_count = df_copy1.filter(F.col(\"vote_average\").isNull()).count()\n",
    "\n",
    "# 4. print results\n",
    "print(f\"Rows matching vote_count==0 AND vote_average < 1: {flagged_count}\")\n",
    "print(f\"Rows with vote_average set to null:       {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3684195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert only the needed columns to Pandas to avoid memory issues\n",
    "df_plot = df_copy1.select(\"vote_average\", \"vote_count\").dropna().toPandas()\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot vote_average histogram\n",
    "sns.histplot(df_plot['vote_average'], bins=20, kde=True, ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Distribution of Vote Average')\n",
    "axes[0].set_xlabel('Vote Average')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Plot vote_count histogram (log scale)\n",
    "sns.histplot(df_plot['vote_count'], bins=50, kde=False, ax=axes[1], color='salmon')\n",
    "axes[1].set_title('Distribution of Vote Count')\n",
    "axes[1].set_xlabel('Vote Count')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_yscale('log')  # log scale for better readability of skewed data\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27332fe",
   "metadata": {},
   "source": [
    "%md\n",
    "The distribution for `vote_average` looks almost normal, with a bump around 6, which aligns with typical average ratings on platforms like IMDb or TMDB. Additionally, almost 1000 movies have a perfect score of 10.\n",
    "\n",
    "When it comes to `vote_count`, there's a long tail of movies with thousands of votes, likely the popular ones. However, a big portion of movies did not receive any votes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d796bf1",
   "metadata": {},
   "source": [
    "%md\n",
    "### RELEASE_DATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7b9886",
   "metadata": {},
   "source": [
    "%md\n",
    "First, we need to handle the data type of `release_date`. Upon initial feature investigation, we realized that some dates might be in US formats and some in EU. We will need to hadle it appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7c06c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of null, empty-string, and \"None\"/\"none\" counts for release_date\n",
    "total_rows    = df_copy1.count()\n",
    "null_count    = df_copy1.filter(col(\"release_date\").isNull()).count()\n",
    "empty_count   = df_copy1.filter((col(\"release_date\") == \"\") | (col(\"release_date\") == \" \")).count()\n",
    "none_count    = df_copy1.filter(col(\"release_date\").isin(\"None\", \"none\")).count()\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Null release_date values          : {null_count}\")\n",
    "print(f\"Empty-string release_date values  : {empty_count}\")\n",
    "print(f\"'None'/'none' release_date values : {none_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c157436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable legacy date parser to avoid SparkUpgradeException on ambiguous dates\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# define possible parsers\n",
    "date_eu  = to_date(col(\"release_date\"), \"dd/MM/yyyy\")\n",
    "date_us  = to_date(col(\"release_date\"), \"MM/dd/yyyy\")\n",
    "date_iso = to_date(col(\"release_date\"), \"yyyy-MM-dd\")\n",
    "\n",
    "# preview 5 rows of each “bucket”\n",
    "print(\"=== EU format samples ===\")\n",
    "df_copy1.filter(date_eu.isNotNull()) \\\n",
    "  .select(\"release_date\") \\\n",
    "  .distinct() \\\n",
    "  .show(5, truncate=False)\n",
    "\n",
    "print(\"=== US format samples ===\")\n",
    "df_copy1.filter(date_eu.isNull() & date_us.isNotNull()) \\\n",
    "  .select(\"release_date\") \\\n",
    "  .distinct() \\\n",
    "  .show(5, truncate=False)\n",
    "\n",
    "print(\"=== ISO format samples ===\")\n",
    "df_copy1.filter(date_eu.isNull() & date_us.isNull() & date_iso.isNotNull()) \\\n",
    "  .select(\"release_date\") \\\n",
    "  .distinct() \\\n",
    "  .show(5, truncate=False)\n",
    "\n",
    "print(\"=== Other/unrecognized samples ===\")\n",
    "df_copy1.filter(date_eu.isNull() & date_us.isNull() & date_iso.isNull()) \\\n",
    "  .select(\"release_date\") \\\n",
    "  .distinct() \\\n",
    "  .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19aed75",
   "metadata": {},
   "source": [
    "%md\n",
    "Turns out we only have dates in eu, us or iso formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0e4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coalesce into a single DateType column\n",
    "df2 = df_copy1.withColumn(\"parsed_date\", coalesce(date_eu, date_us, date_iso))\n",
    "\n",
    "# count how many parsed successfully (i.e. non-null)\n",
    "success_count = df2.filter(col(\"parsed_date\").isNotNull()).count()\n",
    "print(f\"Successfully cast {success_count} dates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0c2fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite the old string column with the new DateType column\n",
    "df2 = df2.drop(\"release_date\").withColumnRenamed(\"parsed_date\", \"release_date\")\n",
    "\n",
    "# Re-assign back to df_copy1 \n",
    "df_copy1 = df2\n",
    "\n",
    "# Display the DataFrame with the parsed release_date column\n",
    "display(df_copy1.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badd2560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the schema to see the data type\n",
    "df_copy1.select(\"release_date\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e7fec0",
   "metadata": {},
   "source": [
    "%md\n",
    "To support future analyses, we will engineer three new features based on the release date: day, month, and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b13e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `release_date` is already a DateType (parsed from EU format)\n",
    "df_copy1 = df_copy1.withColumn(\"release_day\",   dayofmonth(col(\"release_date\"))) \\\n",
    "       .withColumn(\"release_month\", month(col(\"release_date\")))   \\\n",
    "       .withColumn(\"release_year\",  year(col(\"release_date\")))\n",
    "\n",
    "# Confirm the new columns\n",
    "df_copy1.printSchema()\n",
    "display(df_copy1.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a89a10",
   "metadata": {},
   "source": [
    "%md\n",
    "We use a boxplot to visualize the distribution of our data and detect possible outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a3934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = df_copy1.select(\"release_year\").toPandas()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x=df_pd[\"release_year\"])\n",
    "plt.title(\"Boxplot of Release Year\")\n",
    "plt.xlabel(\"Release Year\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ac3d70",
   "metadata": {},
   "source": [
    "%md\n",
    "Based on the boxplot, multiple outliers are identified, with three showing a particularly strong deviation from the rest. As a result, we will set our lower threshold at January 1st, 1910."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b474981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for future release dates (dates beyond today)\n",
    "future_movies = df_copy1.filter(col(\"release_date\") > current_date())\n",
    "\n",
    "# Filter movies released before 1910-01-01\n",
    "old_movies = df_copy1.filter(col(\"release_date\") < lit(\"1910-01-01\"))\n",
    "\n",
    "# Count and print the result\n",
    "count_future = future_movies.count()\n",
    "print(f\"Number of movies released after today: {count_future}\")\n",
    "count_old = old_movies.count()\n",
    "print(f\"Number of movies released before 1910: {count_old}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974789cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few examples \n",
    "display(old_movies.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a95b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 30 movies with selected columns\n",
    "display(future_movies.limit(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c4885f",
   "metadata": {},
   "source": [
    "%md\n",
    "Some movies have \"released\" status but `release_date` in the future. To avoid feeding our model with inconsistent values, we will set the date of these movies to null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e83ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set future release dates to null for movies already marked as “released”\n",
    "df2 = df_copy1.withColumn(\n",
    "    \"release_date\",\n",
    "    when(\n",
    "        (col(\"status\") == \"Released\") &\n",
    "        (col(\"release_date\") > current_date()),\n",
    "        lit(None).cast(\"date\")\n",
    "    ).otherwise(col(\"release_date\"))\n",
    ")\n",
    "\n",
    "# Reassign back to df_copy1 if needed\n",
    "df_copy1 = df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235d2a5",
   "metadata": {},
   "source": [
    "%md\n",
    "To understand the relationship between this variable and our target, we will analyze how the `revenue` behaves throughout the month, across different months of a year, and over multiple years in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbba34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_revenue(df, group_col):\n",
    "    \"\"\"\n",
    "    Compute the average revenue grouped by a specified column.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Spark DataFrame containing a 'revenue' column.\n",
    "        group_col (str): The column name to group by (e.g., 'release_day').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A Pandas DataFrame with the group column and average revenue.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.groupBy(group_col)\n",
    "          .agg(avg(\"revenue\").alias(\"avg_revenue\"))\n",
    "          .orderBy(group_col)\n",
    "          .toPandas()\n",
    "    )\n",
    "\n",
    "def plot_avg_revenue(data, x_col, title, xlabel):\n",
    "    \"\"\"\n",
    "    Plot a line chart of average revenue by a given column.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Pandas DataFrame with the x_col and 'avg_revenue'.\n",
    "        x_col (str): Column name to use as the x-axis.\n",
    "        title (str): Title of the chart.\n",
    "        xlabel (str): Label for the x-axis.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(data[x_col], data[\"avg_revenue\"])\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Avg Revenue\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e0f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute grouped data\n",
    "day_rev = compute_avg_revenue(df_copy1, \"release_day\")\n",
    "month_rev = compute_avg_revenue(df_copy1, \"release_month\")\n",
    "year_rev = compute_avg_revenue(df_copy1, \"release_year\")\n",
    "\n",
    "# Plot the results\n",
    "plot_avg_revenue(day_rev, \"release_day\", \"Avg Revenue by Release Day\", \"Release Day of Month\")\n",
    "plot_avg_revenue(month_rev, \"release_month\", \"Avg Revenue by Release Month\", \"Release Month\")\n",
    "plot_avg_revenue(year_rev, \"release_year\", \"Avg Revenue by Release Year\", \"Release Year\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebffeb2b",
   "metadata": {},
   "source": [
    "%md\n",
    "Since there are only three movies released before 1910, and considering the distribution of the rest of the data, these entries can be treated as outliers. Let's kick them and plot the graphics again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d7b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy1 = df_copy1.filter(\n",
    "    (col(\"release_date\") >= to_date(lit(\"1910-01-01\"))) | col(\"release_date\").isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb5b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute grouped data\n",
    "day_rev = compute_avg_revenue(df_copy1, \"release_day\")\n",
    "month_rev = compute_avg_revenue(df_copy1, \"release_month\")\n",
    "year_rev = compute_avg_revenue(df_copy1, \"release_year\")\n",
    "\n",
    "# Plot the results\n",
    "plot_avg_revenue(day_rev, \"release_day\", \"Avg Revenue by Release Day\", \"Release Day of Month\")\n",
    "plot_avg_revenue(month_rev, \"release_month\", \"Avg Revenue by Release Month\", \"Release Month\")\n",
    "plot_avg_revenue(year_rev, \"release_year\", \"Avg Revenue by Release Year\", \"Release Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd868e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the oldest and most recent films released\n",
    "min_date = df_copy1.select(min(\"release_date\")).collect()[0][0]\n",
    "max_date = df_copy1.select(max(\"release_date\")).collect()[0][0]\n",
    "\n",
    "print(f\"Oldest release date: {min_date}\")\n",
    "print(f\"Most recent release date: {max_date}\")\n",
    "\n",
    "display(df_copy1.filter(col(\"release_date\") == min_date))\n",
    "\n",
    "display(df_copy1.filter(col(\"release_date\") == max_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244cc66",
   "metadata": {},
   "source": [
    "%md\n",
    "The first chart shows the average revenue by the release day of the month. We can observe that movies released on the 7th day tend to perform the best in terms of revenue. In contrast, films released at the very beginning or end of the month tend to generate lower revenues on average. This may reflect strategic scheduling, as mid-month releases might benefit from better marketing alignment and less competition.\n",
    "\n",
    "In the second chart, we analyze average revenue by release month. It becomes clear that movies released around the middle of the year, particularly in June and July, tend to perform significantly better. This pattern aligns with the global summer holiday season, when people are more likely to go to the cinema. Additionally, there is a second peak in revenue towards the end of the year, which is often linked to award-season releases aiming to qualify for events such as the Oscars or Cannes Festival.\n",
    "\n",
    "The third chart presents a historical perspective, showing the evolution of average revenue by release year. There is a clear upward trend, indicating that movies have been generating increasingly higher revenues over time. This makes sense considering the industry's growing investment in production, improvements in visual and sound quality, the increasing scale of global distribution, and rising ticket prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b62c4a2",
   "metadata": {},
   "source": [
    "%md\n",
    "### RUNTIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7250682",
   "metadata": {},
   "source": [
    "%md\n",
    "We will check if there are any inconsistencies in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e40f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of null, empty-string, \"None\"/\"none\", and zero counts for runtime\n",
    "total = df_copy1.count()\n",
    "\n",
    "nulls   = df_copy1.filter(col(\"runtime\").isNull()).count()\n",
    "empties = df_copy1.filter((col(\"runtime\") == \"\") |(col(\"runtime\") == \" \")).count()\n",
    "nones   = df_copy1.filter(col(\"runtime\").isin(\"None\", \"none\")).count()\n",
    "zeros   = df_copy1.filter(col(\"runtime\") == 0).count()\n",
    "\n",
    "print(f\"runtime: total={total}, nulls={nulls}, empty strings={empties}, 'None'/'none' strings={nones}, zero values={zeros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf74da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for negative runtime values\n",
    "negative_runtime_count = df_copy1.filter(col(\"runtime\") < 0).count()\n",
    "print(f\"Number of movies with negative runtime: {negative_runtime_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5717241",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy1.select(\"runtime\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d4ba8",
   "metadata": {},
   "source": [
    "%md\n",
    "Although some \"movies\" can indeed be shorter than a minute (for example, very old movies or ads), in order to clean up our dataset we will assume that movies with 0 value for runtime, actually have this column missing. We will set them to null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows where runtime == 0 before replacing\n",
    "zero_runtime_count = df_copy1.filter(col(\"runtime\") == 0).count()\n",
    "print(f\"Rows with runtime = 0 before nulling: {zero_runtime_count}\")\n",
    "\n",
    "# Replace runtime == 0 with null\n",
    "df_copy1 = df_copy1.withColumn(\n",
    "    \"runtime\",\n",
    "    when(col(\"runtime\") == 0, None).otherwise(col(\"runtime\"))\n",
    ")\n",
    "\n",
    "# Verify how many runtime nulls now (should include the replacements)\n",
    "null_runtime_count = df_copy1.filter(col(\"runtime\").isNull()).count()\n",
    "print(f\"Rows with runtime = null after replacement: {null_runtime_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b5d8fc",
   "metadata": {},
   "source": [
    "%md\n",
    "outliers check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f03dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Q1 and Q3 using approxQuantile\n",
    "q1, q3 = df.approxQuantile(\"runtime\", [0.25, 0.75], 0.01)\n",
    "iqr = q3 - q1\n",
    "\n",
    "# Calculate bounds\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "# Identify outliers\n",
    "outliers_runtime = df_copy1.filter((col(\"runtime\") < lower_bound) | (col(\"runtime\") > upper_bound))\n",
    "df_no_outliers = df_copy1.filter((col(\"runtime\") >= lower_bound) & (col(\"runtime\") <= upper_bound))\n",
    "\n",
    "# Display the number of outliers\n",
    "print(f\"Movies with outlier runtime values: {outliers_runtime.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c01867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whiskers plot to visualize typical runtime and outliers\n",
    "runtime_pd = df_copy1.select(\"runtime\").toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=runtime_pd['runtime'], color='skyblue')\n",
    "\n",
    "plt.title(\"Runtime Distribution with Outliers\", fontsize=16)\n",
    "plt.xlabel(\"Runtime (Minutes)\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4efb1e",
   "metadata": {},
   "source": [
    "%md\n",
    "We can see that some movies in our dataset have extremely long runtimes, with one reaching almost 1000 minutes, that’s nearly 17 hours. This is clearly unrealistic, considering that the average movie length is around 92 minutes (approximately 1 hour and 32 minutes). These values suggest that some of the runtime data may be incorrect or improperly recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991732c7",
   "metadata": {},
   "source": [
    "%md\n",
    "Without outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d67f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame\n",
    "df_no_outliers_pd = df_no_outliers.select(\"runtime\").toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=df_no_outliers_pd['runtime'], color='skyblue')\n",
    "\n",
    "plt.title(\"Runtime Distribution without Outliers\", fontsize=16)\n",
    "plt.xlabel(\"Runtime (Minutes)\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be62225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of films with a runtime exceeding 4 hours (240 minutes)\n",
    "count_above_4h = df_copy1.filter(col(\"runtime\") > 240).count()\n",
    "print(f\"Number of movies with runtime greater than 4 hours: {count_above_4h}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019dc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of observations\n",
    "total_count = df_copy1.count()\n",
    "\n",
    "# Count the number of films with a runtime exceeding 4 hours (240 minutes)\n",
    "above_4h_count = df_copy1.filter(col(\"runtime\") > 240).count()\n",
    "\n",
    "percentage = (above_4h_count / total_count) * 100\n",
    "\n",
    "print(f\"Number of movies with runtime greater than 4 hours: {count_above_4h} which represents {percentage:.2f}% of the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c6e4a1",
   "metadata": {},
   "source": [
    "%md\n",
    "According to the Academy of Motion Picture Arts and Sciences (Oscars), a feature film must have a minimum runtime of 40 minutes, with no defined maximum duration. However, when applying the Interquartile Range (IQR) method to detect outliers in our dataset, we identified over 2,500 movies as outliers. Removing such a large portion of data would significantly reduce the amount of useful information available for building our model.\n",
    "\n",
    "To avoid this loss, and considering that our dataset includes not only feature films but also short films and documentaries, which are also eligible for international awards and nominations, we will define a custom threshold. We will consider valid all movies with a runtime greater than 0 minutes and less than or equal to 240 minutes (4 hours). This range allows us to maintain a clean dataset while still preserving valuable content diversity.\n",
    "\n",
    "By setting this custom threshold, we are discarding only 85 observations, which represent less thann 1% of the entire dataset. This minimal exclusion helps maintain both data quality and volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73336f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers_4h = df_copy1.filter((col(\"runtime\") <= 240) | col(\"runtime\").isNull())\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_outliers_4h_pd = df_outliers_4h.select(\"runtime\").toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=df_outliers_4h_pd['runtime'], color='skyblue')\n",
    "\n",
    "plt.title(\"Runtime Distribution without Outliers\", fontsize=16)\n",
    "plt.xlabel(\"Runtime (Minutes)\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc24c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove the outliers\n",
    "df_copy1 = df_copy1.filter((col(\"runtime\") <= 240) | col(\"runtime\").isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35859949",
   "metadata": {},
   "source": [
    "%md\n",
    "### POPULARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6dc743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of null, empty-string, \"None\"/\"none\", and zero counts for 'popularity' column\n",
    "total = df_copy1.count()\n",
    "\n",
    "nulls   = df_copy1.filter(col(\"popularity\").isNull()).count()\n",
    "empties = df_copy1.filter((col(\"popularity\") == \"\") |(col(\"popularity\") == \" \")).count()\n",
    "nones   = df_copy1.filter(col(\"popularity\").isin(\"None\", \"none\")).count()\n",
    "zeros   = df_copy1.filter(col(\"popularity\") == \"0\").count()\n",
    "\n",
    "print(f\"popularity: total={total}, nulls={nulls}, empty strings={empties}, 'None'/'none' strings={nones}, zero values={zeros}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b97402d",
   "metadata": {},
   "source": [
    "%md\n",
    "Can `popularity` be zero? Let's display some examples of rows with `popularity` = 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f69856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many movies have popularity == 0\n",
    "zero_pop_count = df_copy1.filter(col(\"popularity\") == 0).count()\n",
    "print(f\"Number of movies with popularity = 0: {zero_pop_count}\")\n",
    "\n",
    "# Display some examples of movies with popularity == 0 \n",
    "display(\n",
    "    df_copy1\n",
    "    .filter(col(\"popularity\") == 0)\n",
    "    .limit(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10735571",
   "metadata": {},
   "source": [
    "%md\n",
    "Through these examples, we can clearly observe that films with a popularity score of zero are highly irregular. Many of them have either extremely low or no vote counts, display unrealistic budget values, and most of their categorical (descriptive) features are null. As a result, these entries provide little to no meaningful information and represent films with virtually no traceable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics for popularity\n",
    "stats = df_copy1.agg(\n",
    "    F.min(\"popularity\").alias(\"min_popularity\"),\n",
    "    F.expr(\"percentile_approx(popularity, 0.5)\").alias(\"median_popularity\"),\n",
    "    F.avg(\"popularity\").alias(\"average_popularity\"),\n",
    "    F.max(\"popularity\").alias(\"max_popularity\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Min popularity     : {stats['min_popularity']}\")\n",
    "print(f\"Median popularity  : {stats['median_popularity']}\")\n",
    "print(f\"Average popularity : {stats['average_popularity']:.2f}\")\n",
    "print(f\"Max popularity     : {stats['max_popularity']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb804ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Spark DataFrame\n",
    "pop_filtered = df_copy1.filter(df_copy1['popularity'] > 0)\n",
    "\n",
    "# Convert to Pandas\n",
    "pop_filtered_pd = pop_filtered.select('popularity').toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(pop_filtered_pd['popularity'], kde=True, color='skyblue', bins=30)\n",
    "\n",
    "plt.xlim(0, 300)\n",
    "plt.title(\"Distribution of Popularity\", fontsize=16)\n",
    "plt.xlabel(\"Popularity\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c299ac",
   "metadata": {},
   "source": [
    "%md\n",
    "let's analyse the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae45d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x=pop_filtered_pd[\"popularity\"], color='skyblue')\n",
    "plt.title(\"Boxplot of Popularity\", fontsize=16)\n",
    "plt.xlabel(\"Popularity\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4de72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the total films\n",
    "total_movies = df_copy1.count()\n",
    "\n",
    "# # Count the number of films with a popularity > 100\n",
    "count_above_100 = df_copy1.filter(col(\"popularity\") > 100).count()\n",
    "\n",
    "# percentage\n",
    "percentage_above_100 = (count_above_100 / total_movies) * 100\n",
    "\n",
    "print(f\"Number of movies with popularity greater than 100: {count_above_100}\")\n",
    "print(f\"That represents approximately {percentage_above_100:.2f}% of the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_filtered_100_pd = df_copy1.filter(col(\"popularity\") <= 100).select(\"popularity\").toPandas()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x=pop_filtered_100_pd[\"popularity\"], color='skyblue')\n",
    "plt.title(\"Boxplot of Popularity (≤ 100)\", fontsize=16)\n",
    "plt.xlabel(\"Popularity\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba61fbe7",
   "metadata": {},
   "source": [
    "%md\n",
    "If we consider a threshold of 100 for the popularity variable,  treating it as if it were scaled similarly to a percentage (0 to 100), we would be discarding 125 observations, which represent less than 1% of our dataset. As such, their presence has minimal impact on the overall analysis, making their removal both justified and necessary.\n",
    "\n",
    "In the second boxplot, we can observe the distribution of films according to this new popularity threshold, which helps us better visualize the central tendency and variability of this feature without the distortion caused by extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d742e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove them\n",
    "df_copy1 = df_copy1.filter((col(\"popularity\") <= 100) | col(\"popularity\").isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84079b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to chck again our new stats\n",
    "stats = df_copy1.agg(\n",
    "    F.min(\"popularity\").alias(\"min_popularity\"),\n",
    "    F.expr(\"percentile_approx(popularity, 0.5)\").alias(\"median_popularity\"),\n",
    "    F.avg(\"popularity\").alias(\"average_popularity\"),\n",
    "    F.max(\"popularity\").alias(\"max_popularity\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Min popularity     : {stats['min_popularity']}\")\n",
    "print(f\"Median popularity  : {stats['median_popularity']}\")\n",
    "print(f\"Average popularity : {stats['average_popularity']:.2f}\")\n",
    "print(f\"Max popularity     : {stats['max_popularity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f09a84",
   "metadata": {},
   "source": [
    "%md\n",
    "## Save the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ffefce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_copy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eabbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save our dataset\n",
    "df.write.mode(\"overwrite\").parquet(\"/FileStore/tables/Imdb_Movie_Dataset-4.csv\")\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
